networks:
  my_network:
    driver: bridge
  k6:
    driver: bridge
  grafana:
    driver: bridge

services:
  # Elasticsearch 서비스 추가
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.2
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false # 보안 인증 비활성화
      - xpack.security.http.ssl.enabled=false # SSL 비활성화
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # JVM 메모리 설정
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - my_network
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://elasticsearch:9200" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always

  docker-kibana:
    image: docker.elastic.co/kibana/kibana:8.17.2
    container_name: docker-kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - XPACK_ENCRYPTED_SAVED_OBJECTS_ENCRYPTION_KEY=hello-myname-is-kwon-oh-hyun123456
    ports:
      - 5601:5601
    networks:
      - my_network
    depends_on:
      - elasticsearch
    restart: always

  # Kafka (Zookeeper 없이 KRaft 모드 실행)
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: controller,broker # zookeeper없이 Kafka만으로 컨트롤하려면 controller가 필수이다.
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093 # Controller 후보 지정 -> 브로커 1번이 9093포트에서 컨트롤러역할 수행
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      #	 클라이언트(Publisher, Consumer) → 9092에서 접속
      #	 Kafka 브로커 내부 통신(컨트롤러) → 9093에서 통신
      KAFKA_LISTENERS: PLAINTEXT://kafka:9092,CONTROLLER://kafka:9093

      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 # Kafka 브로커가 외부에서 접근할 수 있도록 9092 포트 광고
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT # Kafka 브로커 간 통신을 PLAINTEXT(암호화 X) 방식으로 설정
      KAFKA_LOG_DIRS: /var/lib/kafka/data # Kafka의 데이터를 저장할 디렉토리 지정 (/var/lib/kafka/data)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      # docker exec -it kafka kafka-topics --bootstrap-server kafka:9092 --create --topic test-topic --partitions 3 --replication-factor 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CLUSTER_ID: X_tpfbxmSoqw8cU-BYoVJA # ClusterID생성 docker exec -it kafka kafka-storage random-uuid, KRaft모드에서는 필수
      KAFKA_DELETE_TOPIC_ENABLE: "true"  # Kafka에서 토픽 삭제 허용
    networks:
      - my_network

#    volumes:
#      - kafka_data:/var/lib/kafka/data
  # Nginx (로드밸런서)
  nginx:
    image: nginx:latest
    container_name: nginx_server
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - springboot1
      - springboot2
      - springboot3

    networks:
      - my_network

  # Spring Boot 서비스 1
  springboot1:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: springboot1
    ports:
      - "8081:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPRING_DATASOURCE_URL=jdbc:mysql://host.docker.internal:3306/sys
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=1234

    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      elasticsearch:
        condition: service_healthy  # Elasticsearch가 정상 실행된 후 실행
      kafka:
        condition: service_started  # Kafka 컨테이너가 시작된 후 실행
    networks:
      - my_network

  # Spring Boot 서비스 2
  springboot2:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: springboot2
    ports:
      - "8082:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPRING_DATASOURCE_URL=jdbc:mysql://host.docker.internal:3306/sys
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=1234
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      elasticsearch:
        condition: service_healthy  # Elasticsearch가 정상 실행된 후 실행
      kafka:
        condition: service_started  # Kafka 컨테이너가 시작된 후 실행
    networks:
      - my_network

  # Spring Boot 서비스 3
  springboot3:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: springboot3
    ports:
      - "8083:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPRING_DATASOURCE_URL=jdbc:mysql://host.docker.internal:3306/sys
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=1234
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      elasticsearch:
        condition: service_healthy  # Elasticsearch가 정상 실행된 후 실행
      kafka:
        condition: service_started  # Kafka 컨테이너가 시작된 후 실행
    networks:
      - my_network
#  # Spring Boot 서비스 4
#  springboot4:
#    build:
#      context: .
#      dockerfile: Dockerfile
#    container_name: springboot4
#    ports:
#      - "8084:8080"
#    environment:
#      - SPRING_PROFILES_ACTIVE=prod
#      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
#      - SPRING_DATASOURCE_URL=jdbc:mysql://host.docker.internal:3306/sys
#      - SPRING_DATASOURCE_USERNAME=root
#      - SPRING_DATASOURCE_PASSWORD=1234
#    extra_hosts:
#      - "host.docker.internal:host-gateway"
#    depends_on:
#      elasticsearch:
#        condition: service_healthy  # Elasticsearch가 정상 실행된 후 실행
#      kafka:
#        condition: service_started  # Kafka 컨테이너가 시작된 후 실행
#    networks:
#      - my_network
#
#  # Spring Boot 서비스 5
#  springboot5:
#    build:
#      context: .
#      dockerfile: Dockerfile
#    container_name: springboot5
#    ports:
#      - "8085:8080"
#    environment:
#      - SPRING_PROFILES_ACTIVE=prod
#      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
#      - SPRING_DATASOURCE_URL=jdbc:mysql://host.docker.internal:3306/sys
#      - SPRING_DATASOURCE_USERNAME=root
#      - SPRING_DATASOURCE_PASSWORD=1234
#    extra_hosts:
#      - "host.docker.internal:host-gateway"
#    depends_on:
#      elasticsearch:
#        condition: service_healthy  # Elasticsearch가 정상 실행된 후 실행
#      kafka:
#        condition: service_started  # Kafka 컨테이너가 시작된 후 실행
#    networks:
#      - my_network

  influxdb:
    image: influxdb:1.8
    networks:
      - k6
      - grafana
    ports:
      - "8086:8086"
    environment:
      - INFLUXDB_DB=k6

  grafana:
    image: grafana/grafana:9.3.8
    networks:
      - grafana
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_BASIC_ENABLED=false
    volumes:
      - ./grafana:/etc/grafana/provisioning/

  k6:
    image: grafana/k6:latest
    networks:
      - k6
      - my_network
    ports:
      - "6565:6565"
    environment:
      - K6_OUT=influxdb=http://influxdb:8086/k6
    volumes:
      - ./scripts/k6-test.js:/scripts/k6-test.js:ro

volumes:
  elasticsearch_data:

